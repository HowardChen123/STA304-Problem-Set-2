---
title: "STA304 Problem Set 2"
author: "Names of your Group Members"
date: "October 19, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Title of your Report

# Name(s) of Author(s) 
# Date

## Abstract

Here is where you give a brief (one paragraph overview of your entire paper). This should include some background/introduction, some methodology, results and conclusions.

## Introduction

Here is where you should give insight into the setting and introduce the goal of the analysis. Here you can introduce ideas and basic concepts regarding the study setting and the potential model. Again, this is the introduction, so you should be explaining the importance of the work that is ahead and hopefully build some suspense for the reader. You can also highlight what will be included in the subsequent sections.

## Data

Introduce the data, explain why it was selected. Make sure to comment on important features and highlight any potential drawbacks to the data.

### Data Cleaning
```{r}
library(tidyverse)
gss_df <- read.csv("gss.csv")
```
```{r}
gss_df <- gss_df %>%
  filter(marital_status %in% c("Married", "Living common-Law"))
gss_df <- gss_df %>% 
  mutate(hasSameEducation = education == partner_education)
gss_df <- gss_df[!is.na(gss_df$hasSameEducation), ]
```

```{r}
final_data <- gss_df %>%
                select(caseid, hasSameEducation, age_first_child, total_children, 
                       feelings_life, place_birth_canada, province, education, 
                       hh_type, partner_birth_province, average_hours_worked, 
                       self_rated_health, self_rated_mental_health, income_family, 
                       income_respondent)
final_data <- na.omit(final_data)
```

```{r}
set.seed(1005107457)
test_id = sample(final_data$caseid, nrow(final_data)/2)
train_df = final_data[!final_data$caseid %in% test_id, ]
test_df = final_data[final_data$caseid %in% test_id, ]
```

Our final dataset contains 5100 observations and each of them contains characteristics of modern Canadian household. Data is splitted into a training set and a testing set where each of them contains 50% (randomly sampled) of the original data. 

## Model

Logistic regression was chosen to fit the relationship between various Canadian household features and our target: whether or not the partners have the same education level. The model was chosen because the target variable is binary, and the logit link is appropriate for a target variable that follows a binomial distribution.

### Base Model
```{r, echo=FALSE, message=FALSE, warning=FALSE}
base_model = glm(hasSameEducation ~ age_first_child+as.factor(total_children)+
                  as.factor(feelings_life)+as.factor(place_birth_canada)+
                   as.factor(province)+as.factor(education)+as.factor(hh_type)+
                   as.factor(partner_birth_province)+as.factor(average_hours_worked)+
                   as.factor(self_rated_health)+as.factor(self_rated_mental_health)+
                   as.factor(income_family)+as.factor(income_respondent), 
                 family = binomial, data = train_df)
```

We begin by building a base model that includes all the predictors. There are a lot of insignificant predictors and hence, we intended to perform variable selection to explain the variability with a simpler model. 

### Variable Selection
```{r, echo=FALSE, warning=FALSE}
## AIC ##
step(base_model,
     direction = c("forward"), trace = 0, k = 2)
step(base_model,
     direction = c("backward"), trace = 0, k = 2)
## BIC ##
step(base_model,
     direction = c("forward"), trace = 0, k = log(nrow(train_df)))
step(base_model,
     direction = c("backward"), trace = 0, k = log(nrow(train_df)))
```
```{r, echo=FALSE, warning=FALSE}
forward_aic <- glm(formula = hasSameEducation ~ age_first_child + as.factor(total_children) + 
    as.factor(feelings_life) + as.factor(place_birth_canada) + 
    as.factor(province) + as.factor(education) + as.factor(hh_type) + 
    as.factor(partner_birth_province) + as.factor(average_hours_worked) + 
    as.factor(self_rated_health) + as.factor(self_rated_mental_health) + 
    as.factor(income_family) + as.factor(income_respondent), 
    family = binomial, data = train_df)
```

```{r, echo=FALSE, warning=FALSE}
backward_aic <- glm(formula = hasSameEducation ~ as.factor(feelings_life) + as.factor(place_birth_canada) + 
    as.factor(education) + as.factor(self_rated_mental_health), 
    family = binomial, data = train_df)
```

```{r, echo=FALSE, warning=FALSE}
backward_bic <- glm(formula = hasSameEducation ~ as.factor(education), family = binomial, 
    data = train_df)
```

We performed variable selection through forward and backward AIC/BIC selection. Note that we have obtained the same model through the forward stepwise AIC method and the forward stepwise BIC method. To decide which model has a better fit, we performed the likelihood ratio test (forward stepwise AIC model as base model) and measured their prediction power through AUC value. We concluded that the backward stepwise AIC model performs the best in terms of Goodness of Fit and it is simple enough to provide good predictions.

```{r, message=FALSE, echo=FALSE}
library(pROC)
library(ROCR)
calculate_roc <- function(model, data){
  p <- predict(model, type = "response")
  roc_logit <- roc(data$hasSameEducation ~ p)
  ## The True Positive Rate ##
  TPR <- roc_logit$sensitivities
  ## The False Positive Rate ##
  FPR <- 1 - roc_logit$specificities
  plot(FPR, TPR, xlim = c(0,1), ylim = c(0,1), type = 'l', lty = 1, lwd = 2,col = 'red')
  abline(a = 0, b = 1, lty = 2, col = 'blue')
  text(0.7,0.4,label = paste("AUC = ", round(auc(roc_logit),2)))
  print(paste("AUC: ",auc(roc_logit)))
}
```

```{r, echo=FALSE, message=FALSE, results='hide'}
library(epiDisplay)
lrtest(forward_aic, backward_aic)
lrtest(backward_bic, backward_aic)
```

```{r}
library(pander)
candidate_comparison <- data.frame("Candidate Model" = c("Forward Stepwise AIC", "Backward Stepwise AIC", "Backward Stepwise BIC"), "Number of Predictors" = c("13", "4", "1"), "Likelihood Ratio Test" = c("1", "0.1690", "0.0028"), "AUC" = c("0.91", "0.90", "0.88"))
set.alignment('centre')
pander(candidate_comparison, split.table=Inf, split.cells = c(2,1,30,45), style="multiline")
```

### Validation

We validated our model through its prediction accuracy in the test set. We observed that our model has an AUC value of 0.89, which is close to its AUC value with the training set (0.9). Therefore, our model generalizes well and provides good predictions.

```{r, warning=FALSE, echo=FALSE}
test_model <- glm(formula = hasSameEducation ~ as.factor(feelings_life) + as.factor(place_birth_canada) + as.factor(education) + as.factor(self_rated_mental_health), 
    family = binomial, data = test_df)
```

```{r, echo=FALSE}
calculate_roc(test_model, test_df)
```

### Influential Points

We identified influential points through Cook's Distance to assess observations' influence over the model fit. We observed that there are two observations that have significant effect on the model fit, hence we removed them and validate the model again. We observe that the model performs significantly better after removing the influential points. 

```{r}
library(car)
plot(cooks.distance(test_model))
```

```{r, warning=FALSE, echo=FALSE}
HighLeverage <- cooks.distance(test_model) > (4/nrow(test_df))
LargeResiduals <- rstudent(test_model) > 3
test_df_removed <- test_df[!HighLeverage & !LargeResiduals,]
updated_model <- glm(hasSameEducation ~ as.factor(feelings_life) + as.factor(place_birth_canada) + 
      as.factor(education) + as.factor(self_rated_mental_health), family = binomial, data = test_df_removed)
```

```{r}
calculate_roc(updated_model, test_df_removed)
```

### Diagnostics

In this section, we will verify the model assumptions. 

```{r}
library(arm)
binnedplot(fitted(updated_model), 
           residuals(updated_model, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Fitted Values", 
           ylab = "Average residual",
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")
```

## Results

```{r, echo=FALSE}
summary(updated_model)
```

Here you will include all results. This includes descriptive statistics, graphs, figures, tables, and model results. Please ensure that everything is well formatted and in a report style. You must also provide an explanation of the results in this section. You can overflow to an Appendix if needed. 

Please ensure that everything is well labelled. So if you have multiple histograms and plots, calling them Figure 1, 2, 3, etc. and referencing them as Figure 1, Figure 2, etc. in your report will be expected. The reader should not get lost in a sea of information. Make sure to have the results be clean, well formatted and digestible.

## Discussion

Here you will discuss conclusions drawn from the results and comment on how it relates to the original goal of the study (which was specified in the Introduction).

# Weaknesses

Here we discuss weaknesses of the study, data, analysis, etc. You can also discuss areas for improvement.

# Next Steps

Here you discuss subsequent work to be done after this report. This can include next steps in terms of statistical analysis (perhaps there is a more efficient algorithm available, or perhaps there is a caveat in the data that would allow for some new technique). Future steps should also be specified in terms of the study setting (eg. including a follow-up survey on something, or a subsequent study that would complement the conclusions of your report).


## References



